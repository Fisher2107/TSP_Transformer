{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer Network for the Traveling Salesman Problem\n",
    "\n",
    "Xavier Bresson, Thomas Laurent, Feb 2021<br>\n",
    "\n",
    "Arxiv : https://arxiv.org/pdf/2103.03012.pdf<br>\n",
    "Talk : https://ipam.wistia.com/medias/0jrweluovs<br>\n",
    "Slides : https://t.co/ySxGiKtQL5<br>\n",
    "\n",
    "This code trains the transformer network by reinforcement learning.<br>\n",
    "Use the beam search code to test the trained network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18942/2912911153.py:18: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('png2x','pdf')\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# Libs\n",
    "###################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "# visualization \n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats, clear_output\n",
    "set_matplotlib_formats('png2x','pdf')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "try: \n",
    "    import networkx as nx\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    from concorde.tsp import TSPSolver # !pip install -e pyconcorde\n",
    "except:\n",
    "    pass\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/TSP_Transformer\n",
      "GPU name: Tesla T4, gpu_id: 0,1\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# Hardware : CPU / GPU(s)\n",
    "###################\n",
    "if \"TSP\" not in os.getcwd():\n",
    "    os.chdir(\"TSP_Transformer\")\n",
    "print(os.getcwd())\n",
    "device = torch.device(\"cpu\"); gpu_id = -1 # select CPU\n",
    "\n",
    "#gpu_id = '0' # select a single GPU  \n",
    "gpu_id = '0,1' # select multiple GPUs  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)  \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('GPU name: {:s}, gpu_id: {:s}'.format(torch.cuda.get_device_name(0),gpu_id))   \n",
    "    \n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nb_nodes': 100, 'bsz': 256, 'dim_emb': 128, 'dim_ff': 512, 'dim_input_nodes': 2, 'nb_layers_encoder': 6, 'nb_layers_decoder': 2, 'nb_heads': 8, 'nb_epochs': 10000, 'nb_batch_per_epoch': 1500, 'nb_batch_eval': 20, 'gpu_id': '0,1', 'lr': 0.0001, 'tol': 0.001, 'batchnorm': True, 'max_len_PE': 1000}\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# Hyper-parameters\n",
    "###################\n",
    "\n",
    "class DotDict(dict):\n",
    "    def __init__(self, **kwds):\n",
    "        self.update(kwds)\n",
    "        self.__dict__ = self\n",
    "        \n",
    "args = DotDict()\n",
    "args.nb_nodes = 20 # TSP20\n",
    "args.nb_nodes = 50 # TSP50\n",
    "args.nb_nodes = 100 # TSP100\n",
    "args.bsz = 512 #512 TSP20 TSP50\n",
    "args.dim_emb = 128\n",
    "args.dim_ff = 512\n",
    "args.dim_input_nodes = 2\n",
    "args.nb_layers_encoder = 6\n",
    "args.nb_layers_decoder = 2\n",
    "args.nb_heads = 8\n",
    "args.nb_epochs = 10000\n",
    "args.nb_batch_per_epoch = 1500#2500\n",
    "args.nb_batch_eval = 20\n",
    "args.gpu_id = gpu_id\n",
    "args.lr = 1e-4\n",
    "args.tol = 1e-3\n",
    "args.batchnorm = True  # if batchnorm=True  than batch norm is used\n",
    "#args.batchnorm = False # if batchnorm=False than layer norm is used\n",
    "args.max_len_PE = 1000\n",
    "\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb of nodes : 100\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# Small test set for quick algorithm comparison\n",
    "# Note : this can be removed\n",
    "###################\n",
    "\n",
    "save_1000tsp = True\n",
    "save_1000tsp = False\n",
    "if save_1000tsp:\n",
    "    bsz = 1000\n",
    "    x = torch.rand(bsz, args.nb_nodes, args.dim_input_nodes, device='cpu') \n",
    "    print(x.size(),x[0])\n",
    "    data_dir = os.path.join(\"data\")\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    if args.nb_nodes==20 : torch.save({ 'x': x, }, '{}.pkl'.format(data_dir + \"/1000tsp20\"))\n",
    "    if args.nb_nodes==50 : torch.save({ 'x': x, }, '{}.pkl'.format(data_dir + \"/1000tsp50\"))\n",
    "    if args.nb_nodes==100 : torch.save({ 'x': x, }, '{}.pkl'.format(data_dir + \"/1000tsp100\"))\n",
    "\n",
    "checkpoint = None\n",
    "if args.nb_nodes==20 : checkpoint = torch.load(\"data/1000tsp20.pkl\")\n",
    "if args.nb_nodes==50 : checkpoint = torch.load(\"data/1000tsp50.pkl\")\n",
    "if args.nb_nodes==100 : checkpoint = torch.load(\"data/1000tsp100.pkl\")\n",
    "if checkpoint is not None:\n",
    "    x_1000tsp = checkpoint['x'].to(device)\n",
    "    n = x_1000tsp.size(1)\n",
    "    print('nb of nodes :',n)\n",
    "else:\n",
    "    x_1000tsp = torch.rand(1000, args.nb_nodes, args.dim_input_nodes, device='cpu')\n",
    "    n = x_1000tsp.size(1)\n",
    "    print('nb of nodes :',n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'nb_nodes': 100, 'bsz': 256, 'dim_emb': 128, 'dim_ff': 512, 'dim_input_nodes': 2, 'nb_layers_encoder': 6, 'nb_layers_decoder': 2, 'nb_heads': 8, 'nb_epochs': 10000, 'nb_batch_per_epoch': 1500, 'nb_batch_eval': 20, 'gpu_id': '0,1', 'lr': 0.0001, 'tol': 0.001, 'batchnorm': True, 'max_len_PE': 1000}\n",
      "\n",
      "start train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘logs’: File exists\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# Network definition\n",
    "# Notation : \n",
    "#            bsz : batch size\n",
    "#            nb_nodes : number of nodes/cities\n",
    "#            dim_emb : embedding/hidden dimension\n",
    "#            nb_heads : nb of attention heads\n",
    "#            dim_ff : feed-forward dimension\n",
    "#            nb_layers : number of encoder/decoder layers\n",
    "###################\n",
    "def compute_tour_length(x, tour): \n",
    "    \"\"\"\n",
    "    Compute the length of a batch of tours\n",
    "    Inputs : x of size (bsz, nb_nodes, 2) batch of tsp tour instances\n",
    "             tour of size (bsz, nb_nodes) batch of sequences (node indices) of tsp tours\n",
    "    Output : L of size (bsz,)             batch of lengths of each tsp tour\n",
    "    \"\"\"\n",
    "    bsz = x.shape[0]\n",
    "    nb_nodes = x.shape[1]\n",
    "    arange_vec = torch.arange(bsz, device=x.device)\n",
    "    first_cities = x[arange_vec, tour[:,0], :] # size(first_cities)=(bsz,2)\n",
    "    previous_cities = first_cities\n",
    "    L = torch.zeros(bsz, device=x.device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(1,nb_nodes):\n",
    "            current_cities = x[arange_vec, tour[:,i], :] \n",
    "            L += torch.sum( (current_cities - previous_cities)**2 , dim=1 )**0.5 # dist(current, previous node) \n",
    "            previous_cities = current_cities\n",
    "        L += torch.sum( (current_cities - first_cities)**2 , dim=1 )**0.5 # dist(last, first node)  \n",
    "    return L\n",
    "\n",
    "\n",
    "class Transformer_encoder_net(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder network based on self-attention transformer\n",
    "    Inputs :  \n",
    "      h of size      (bsz, nb_nodes+1, dim_emb)    batch of input cities\n",
    "    Outputs :  \n",
    "      h of size      (bsz, nb_nodes+1, dim_emb)    batch of encoded cities\n",
    "      score of size  (bsz, nb_nodes+1, nb_nodes+1) batch of attention scores\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_layers, dim_emb, nb_heads, dim_ff, batchnorm):\n",
    "        super(Transformer_encoder_net, self).__init__()\n",
    "        assert dim_emb == nb_heads* (dim_emb//nb_heads) # check if dim_emb is divisible by nb_heads\n",
    "        self.MHA_layers = nn.ModuleList( [nn.MultiheadAttention(dim_emb, nb_heads) for _ in range(nb_layers)] )\n",
    "        self.linear1_layers = nn.ModuleList( [nn.Linear(dim_emb, dim_ff) for _ in range(nb_layers)] )\n",
    "        self.linear2_layers = nn.ModuleList( [nn.Linear(dim_ff, dim_emb) for _ in range(nb_layers)] )   \n",
    "        if batchnorm:\n",
    "            self.norm1_layers = nn.ModuleList( [nn.BatchNorm1d(dim_emb) for _ in range(nb_layers)] )\n",
    "            self.norm2_layers = nn.ModuleList( [nn.BatchNorm1d(dim_emb) for _ in range(nb_layers)] )\n",
    "        else:\n",
    "            self.norm1_layers = nn.ModuleList( [nn.LayerNorm(dim_emb) for _ in range(nb_layers)] )\n",
    "            self.norm2_layers = nn.ModuleList( [nn.LayerNorm(dim_emb) for _ in range(nb_layers)] )\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_heads = nb_heads\n",
    "        self.batchnorm = batchnorm\n",
    "        \n",
    "    def forward(self, h):      \n",
    "        # PyTorch nn.MultiheadAttention requires input size (seq_len, bsz, dim_emb) \n",
    "        h = h.transpose(0,1) # size(h)=(nb_nodes, bsz, dim_emb)  \n",
    "        # L layers\n",
    "        for i in range(self.nb_layers):\n",
    "            h_rc = h # residual connection, size(h_rc)=(nb_nodes, bsz, dim_emb)\n",
    "            h, score = self.MHA_layers[i](h, h, h) # size(h)=(nb_nodes, bsz, dim_emb), size(score)=(bsz, nb_nodes, nb_nodes)\n",
    "            # add residual connection\n",
    "            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            if self.batchnorm:\n",
    "                # Pytorch nn.BatchNorm1d requires input size (bsz, dim, seq_len)\n",
    "                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = self.norm1_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            else:\n",
    "                h = self.norm1_layers[i](h)       # size(h)=(nb_nodes, bsz, dim_emb) \n",
    "            # feedforward\n",
    "            h_rc = h # residual connection\n",
    "            h = self.linear2_layers[i](torch.relu(self.linear1_layers[i](h)))\n",
    "            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            if self.batchnorm:\n",
    "                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = self.norm2_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            else:\n",
    "                h = self.norm2_layers[i](h) # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "        # Transpose h\n",
    "        h = h.transpose(0,1) # size(h)=(bsz, nb_nodes, dim_emb)\n",
    "        return h, score\n",
    "    \n",
    "\n",
    "def myMHA(Q, K, V, nb_heads, mask=None, clip_value=None):\n",
    "    \"\"\"\n",
    "    Compute multi-head attention (MHA) given a query Q, key K, value V and attention mask :\n",
    "      h = Concat_{k=1}^nb_heads softmax(Q_k^T.K_k).V_k \n",
    "    Note : We did not use nn.MultiheadAttention to avoid re-computing all linear transformations at each call.\n",
    "    Inputs : Q of size (bsz, dim_emb, 1)                batch of queries\n",
    "             K of size (bsz, dim_emb, nb_nodes+1)       batch of keys\n",
    "             V of size (bsz, dim_emb, nb_nodes+1)       batch of values\n",
    "             mask of size (bsz, nb_nodes+1)             batch of masks of visited cities\n",
    "             clip_value is a scalar \n",
    "    Outputs : attn_output of size (bsz, 1, dim_emb)     batch of attention vectors\n",
    "              attn_weights of size (bsz, 1, nb_nodes+1) batch of attention weights\n",
    "    \"\"\"\n",
    "    bsz, nb_nodes, emd_dim = K.size() #  dim_emb must be divisable by nb_heads\n",
    "    if nb_heads>1:\n",
    "        # PyTorch view requires contiguous dimensions for correct reshaping\n",
    "        Q = Q.transpose(1,2).contiguous() # size(Q)=(bsz, dim_emb, 1)\n",
    "        Q = Q.view(bsz*nb_heads, emd_dim//nb_heads, 1) # size(Q)=(bsz*nb_heads, dim_emb//nb_heads, 1)\n",
    "        Q = Q.transpose(1,2).contiguous() # size(Q)=(bsz*nb_heads, 1, dim_emb//nb_heads)\n",
    "        K = K.transpose(1,2).contiguous() # size(K)=(bsz, dim_emb, nb_nodes+1)\n",
    "        K = K.view(bsz*nb_heads, emd_dim//nb_heads, nb_nodes) # size(K)=(bsz*nb_heads, dim_emb//nb_heads, nb_nodes+1)\n",
    "        K = K.transpose(1,2).contiguous() # size(K)=(bsz*nb_heads, nb_nodes+1, dim_emb//nb_heads)\n",
    "        V = V.transpose(1,2).contiguous() # size(V)=(bsz, dim_emb, nb_nodes+1)\n",
    "        V = V.view(bsz*nb_heads, emd_dim//nb_heads, nb_nodes) # size(V)=(bsz*nb_heads, dim_emb//nb_heads, nb_nodes+1)\n",
    "        V = V.transpose(1,2).contiguous() # size(V)=(bsz*nb_heads, nb_nodes+1, dim_emb//nb_heads)\n",
    "    attn_weights = torch.bmm(Q, K.transpose(1,2))/ Q.size(-1)**0.5 # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "    if clip_value is not None:\n",
    "        attn_weights = clip_value * torch.tanh(attn_weights)\n",
    "    if mask is not None:\n",
    "        if nb_heads>1:\n",
    "            mask = torch.repeat_interleave(mask, repeats=nb_heads, dim=0) # size(mask)=(bsz*nb_heads, nb_nodes+1)\n",
    "        #attn_weights = attn_weights.masked_fill(mask.unsqueeze(1), float('-inf')) # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "        attn_weights = attn_weights.masked_fill(mask.unsqueeze(1), float('-1e9')) # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "    attn_weights = torch.softmax(attn_weights, dim=-1) # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "    attn_output = torch.bmm(attn_weights, V) # size(attn_output)=(bsz*nb_heads, 1, dim_emb//nb_heads)\n",
    "    if nb_heads>1:\n",
    "        attn_output = attn_output.transpose(1,2).contiguous() # size(attn_output)=(bsz*nb_heads, dim_emb//nb_heads, 1)\n",
    "        attn_output = attn_output.view(bsz, emd_dim, 1) # size(attn_output)=(bsz, dim_emb, 1)\n",
    "        attn_output = attn_output.transpose(1,2).contiguous() # size(attn_output)=(bsz, 1, dim_emb)\n",
    "        attn_weights = attn_weights.view(bsz, nb_heads, 1, nb_nodes) # size(attn_weights)=(bsz, nb_heads, 1, nb_nodes+1)\n",
    "        attn_weights = attn_weights.mean(dim=1) # mean over the heads, size(attn_weights)=(bsz, 1, nb_nodes+1)\n",
    "    return attn_output, attn_weights\n",
    "    \n",
    "    \n",
    "class AutoRegressiveDecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single decoder layer based on self-attention and query-attention\n",
    "    Inputs :  \n",
    "      h_t of size      (bsz, 1, dim_emb)          batch of input queries\n",
    "      K_att of size    (bsz, nb_nodes+1, dim_emb) batch of query-attention keys\n",
    "      V_att of size    (bsz, nb_nodes+1, dim_emb) batch of query-attention values\n",
    "      mask of size     (bsz, nb_nodes+1)          batch of masks of visited cities\n",
    "    Output :  \n",
    "      h_t of size (bsz, nb_nodes+1)               batch of transformed queries\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_emb, nb_heads):\n",
    "        super(AutoRegressiveDecoderLayer, self).__init__()\n",
    "        self.dim_emb = dim_emb\n",
    "        self.nb_heads = nb_heads\n",
    "        self.Wq_selfatt = nn.Linear(dim_emb, dim_emb)\n",
    "        self.Wk_selfatt = nn.Linear(dim_emb, dim_emb)\n",
    "        self.Wv_selfatt = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W0_selfatt = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W0_att = nn.Linear(dim_emb, dim_emb)\n",
    "        self.Wq_att = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W1_MLP = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W2_MLP = nn.Linear(dim_emb, dim_emb)\n",
    "        self.BN_selfatt = nn.LayerNorm(dim_emb)\n",
    "        self.BN_att = nn.LayerNorm(dim_emb)\n",
    "        self.BN_MLP = nn.LayerNorm(dim_emb)\n",
    "        self.K_sa = None\n",
    "        self.V_sa = None\n",
    "\n",
    "    def reset_selfatt_keys_values(self):\n",
    "        self.K_sa = None\n",
    "        self.V_sa = None\n",
    "        \n",
    "    def forward(self, h_t, K_att, V_att, mask):\n",
    "        bsz = h_t.size(0)\n",
    "        h_t = h_t.view(bsz,1,self.dim_emb) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        # embed the query for self-attention\n",
    "        q_sa = self.Wq_selfatt(h_t) # size(q_sa)=(bsz, 1, dim_emb)\n",
    "        k_sa = self.Wk_selfatt(h_t) # size(k_sa)=(bsz, 1, dim_emb)\n",
    "        v_sa = self.Wv_selfatt(h_t) # size(v_sa)=(bsz, 1, dim_emb)\n",
    "        # concatenate the new self-attention key and value to the previous keys and values\n",
    "        if self.K_sa is None:\n",
    "            self.K_sa = k_sa # size(self.K_sa)=(bsz, 1, dim_emb)\n",
    "            self.V_sa = v_sa # size(self.V_sa)=(bsz, 1, dim_emb)\n",
    "        else:\n",
    "            self.K_sa = torch.cat([self.K_sa, k_sa], dim=1)\n",
    "            self.V_sa = torch.cat([self.V_sa, v_sa], dim=1)\n",
    "        # compute self-attention between nodes in the partial tour\n",
    "        h_t = h_t + self.W0_selfatt( myMHA(q_sa, self.K_sa, self.V_sa, self.nb_heads)[0] ) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        h_t = self.BN_selfatt(h_t.squeeze()) # size(h_t)=(bsz, dim_emb)\n",
    "        h_t = h_t.view(bsz, 1, self.dim_emb) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        # compute attention between self-attention nodes and encoding nodes in the partial tour (translation process)\n",
    "        q_a = self.Wq_att(h_t) # size(q_a)=(bsz, 1, dim_emb)\n",
    "        h_t = h_t + self.W0_att( myMHA(q_a, K_att, V_att, self.nb_heads, mask)[0] ) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        h_t = self.BN_att(h_t.squeeze()) # size(h_t)=(bsz, dim_emb)\n",
    "        h_t = h_t.view(bsz, 1, self.dim_emb) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        # MLP\n",
    "        h_t = h_t + self.W2_MLP(torch.relu(self.W1_MLP(h_t)))\n",
    "        h_t = self.BN_MLP(h_t.squeeze(1)) # size(h_t)=(bsz, dim_emb)\n",
    "        return h_t\n",
    "        \n",
    "        \n",
    "class Transformer_decoder_net(nn.Module): \n",
    "    \"\"\"\n",
    "    Decoder network based on self-attention and query-attention transformers\n",
    "    Inputs :  \n",
    "      h_t of size      (bsz, 1, dim_emb)                            batch of input queries\n",
    "      K_att of size    (bsz, nb_nodes+1, dim_emb*nb_layers_decoder) batch of query-attention keys for all decoding layers\n",
    "      V_att of size    (bsz, nb_nodes+1, dim_emb*nb_layers_decoder) batch of query-attention values for all decoding layers\n",
    "      mask of size     (bsz, nb_nodes+1)                            batch of masks of visited cities\n",
    "    Output :  \n",
    "      prob_next_node of size (bsz, nb_nodes+1)                      batch of probabilities of next node\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_emb, nb_heads, nb_layers_decoder):\n",
    "        super(Transformer_decoder_net, self).__init__()\n",
    "        self.dim_emb = dim_emb\n",
    "        self.nb_heads = nb_heads\n",
    "        self.nb_layers_decoder = nb_layers_decoder\n",
    "        self.decoder_layers = nn.ModuleList( [AutoRegressiveDecoderLayer(dim_emb, nb_heads) for _ in range(nb_layers_decoder-1)] )\n",
    "        self.Wq_final = nn.Linear(dim_emb, dim_emb)\n",
    "        \n",
    "    # Reset to None self-attention keys and values when decoding starts \n",
    "    def reset_selfatt_keys_values(self): \n",
    "        for l in range(self.nb_layers_decoder-1):\n",
    "            self.decoder_layers[l].reset_selfatt_keys_values()\n",
    "            \n",
    "    def forward(self, h_t, K_att, V_att, mask):\n",
    "        for l in range(self.nb_layers_decoder):\n",
    "            K_att_l = K_att[:,:,l*self.dim_emb:(l+1)*self.dim_emb].contiguous()  # size(K_att_l)=(bsz, nb_nodes+1, dim_emb)\n",
    "            V_att_l = V_att[:,:,l*self.dim_emb:(l+1)*self.dim_emb].contiguous()  # size(V_att_l)=(bsz, nb_nodes+1, dim_emb)\n",
    "            if l<self.nb_layers_decoder-1: # decoder layers with multiple heads (intermediate layers)\n",
    "                h_t = self.decoder_layers[l](h_t, K_att_l, V_att_l, mask)\n",
    "            else: # decoder layers with single head (final layer)\n",
    "                q_final = self.Wq_final(h_t)\n",
    "                bsz = h_t.size(0)\n",
    "                q_final = q_final.view(bsz, 1, self.dim_emb)\n",
    "                attn_weights = myMHA(q_final, K_att_l, V_att_l, 1, mask, 10)[1] \n",
    "        prob_next_node = attn_weights.squeeze(1) \n",
    "        return prob_next_node\n",
    "\n",
    "\n",
    "def generate_positional_encoding(d_model, max_len):\n",
    "    \"\"\"\n",
    "    Create standard transformer PEs.\n",
    "    Inputs :  \n",
    "      d_model is a scalar correspoding to the hidden dimension\n",
    "      max_len is the maximum length of the sequence\n",
    "    Output :  \n",
    "      pe of size (max_len, d_model), where d_model=dim_emb, max_len=1000\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "    pe[:,0::2] = torch.sin(position * div_term)\n",
    "    pe[:,1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "    \n",
    "    \n",
    "class TSP_net(nn.Module): \n",
    "    \"\"\"\n",
    "    The TSP network is composed of two steps :\n",
    "      Step 1. Encoder step : Take a set of 2D points representing a fully connected graph \n",
    "                             and encode the set with self-transformer.\n",
    "      Step 2. Decoder step : Build the TSP tour recursively/autoregressively, \n",
    "                             i.e. one node at a time, with a self-transformer and query-transformer. \n",
    "    Inputs : \n",
    "      x of size (bsz, nb_nodes, dim_emb) Euclidian coordinates of the nodes/cities\n",
    "      deterministic is a boolean : If True the salesman will chose the city with highest probability. \n",
    "                                   If False the salesman will chose the city with Bernouilli sampling.\n",
    "    Outputs : \n",
    "      tours of size (bsz, nb_nodes) : batch of tours, i.e. sequences of ordered cities \n",
    "                                      tours[b,t] contains the idx of the city visited at step t in batch b\n",
    "      sumLogProbOfActions of size (bsz,) : batch of sum_t log prob( pi_t | pi_(t-1),...,pi_0 )\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim_input_nodes, dim_emb, dim_ff, nb_layers_encoder, nb_layers_decoder, nb_heads, max_len_PE,\n",
    "                 batchnorm=True):\n",
    "        super(TSP_net, self).__init__()\n",
    "        \n",
    "        self.dim_emb = dim_emb\n",
    "        \n",
    "        # input embedding layer\n",
    "        self.input_emb = nn.Linear(dim_input_nodes, dim_emb)\n",
    "        \n",
    "        # encoder layer\n",
    "        self.encoder = Transformer_encoder_net(nb_layers_encoder, dim_emb, nb_heads, dim_ff, batchnorm)\n",
    "        \n",
    "        # vector to start decoding \n",
    "        self.start_placeholder = nn.Parameter(torch.randn(dim_emb))\n",
    "        \n",
    "        # decoder layer\n",
    "        self.decoder = Transformer_decoder_net(dim_emb, nb_heads, nb_layers_decoder)\n",
    "        self.WK_att_decoder = nn.Linear(dim_emb, nb_layers_decoder* dim_emb) \n",
    "        self.WV_att_decoder = nn.Linear(dim_emb, nb_layers_decoder* dim_emb) \n",
    "        self.PE = generate_positional_encoding(dim_emb, max_len_PE)        \n",
    "        \n",
    "    def forward(self, x, deterministic=False):\n",
    "\n",
    "        # some parameters\n",
    "        bsz = x.shape[0]\n",
    "        nb_nodes = x.shape[1]\n",
    "        zero_to_bsz = torch.arange(bsz, device=x.device) # [0,1,...,bsz-1]\n",
    "\n",
    "        # input embedding layer\n",
    "        h = self.input_emb(x) # size(h)=(bsz, nb_nodes, dim_emb)\n",
    "        \n",
    "        # concat the nodes and the input placeholder that starts the decoding\n",
    "        h = torch.cat([h, self.start_placeholder.repeat(bsz, 1, 1)], dim=1) # size(start_placeholder)=(bsz, nb_nodes+1, dim_emb)\n",
    "        \n",
    "        # encoder layer\n",
    "        h_encoder, _ = self.encoder(h) # size(h)=(bsz, nb_nodes+1, dim_emb)\n",
    "\n",
    "        # list that will contain Long tensors of shape (bsz,) that gives the idx of the cities chosen at time t\n",
    "        tours = []\n",
    "\n",
    "        # list that will contain Float tensors of shape (bsz,) that gives the neg log probs of the choices made at time t\n",
    "        sumLogProbOfActions = []\n",
    "\n",
    "        # key and value for decoder    \n",
    "        K_att_decoder = self.WK_att_decoder(h_encoder) # size(K_att)=(bsz, nb_nodes+1, dim_emb*nb_layers_decoder)\n",
    "        V_att_decoder = self.WV_att_decoder(h_encoder) # size(V_att)=(bsz, nb_nodes+1, dim_emb*nb_layers_decoder)\n",
    "        \n",
    "        # input placeholder that starts the decoding\n",
    "        self.PE = self.PE.to(x.device)\n",
    "        idx_start_placeholder = torch.Tensor([nb_nodes]).long().repeat(bsz).to(x.device)\n",
    "        h_start = h_encoder[zero_to_bsz, idx_start_placeholder, :] + self.PE[0].repeat(bsz,1) # size(h_start)=(bsz, dim_emb)\n",
    "        \n",
    "        # initialize mask of visited cities\n",
    "        mask_visited_nodes = torch.zeros(bsz, nb_nodes+1, device=x.device).bool() # False\n",
    "        mask_visited_nodes[zero_to_bsz, idx_start_placeholder] = True\n",
    "        \n",
    "        # clear key and val stored in the decoder\n",
    "        self.decoder.reset_selfatt_keys_values()\n",
    "\n",
    "        # construct tour recursively\n",
    "        h_t = h_start\n",
    "        for t in range(nb_nodes):\n",
    "            \n",
    "            # compute probability over the next node in the tour\n",
    "            prob_next_node = self.decoder(h_t, K_att_decoder, V_att_decoder, mask_visited_nodes) # size(prob_next_node)=(bsz, nb_nodes+1)\n",
    "            \n",
    "            # choose node with highest probability or sample with Bernouilli \n",
    "            if deterministic:\n",
    "                idx = torch.argmax(prob_next_node, dim=1) # size(query)=(bsz,)\n",
    "            else:\n",
    "                idx = Categorical(prob_next_node).sample() # size(query)=(bsz,)\n",
    "            \n",
    "            # compute logprobs of the action items in the list sumLogProbOfActions   \n",
    "            ProbOfChoices = prob_next_node[zero_to_bsz, idx] \n",
    "            sumLogProbOfActions.append( torch.log(ProbOfChoices) )  # size(query)=(bsz,)\n",
    "\n",
    "            # update embedding of the current visited node\n",
    "            h_t = h_encoder[zero_to_bsz, idx, :] # size(h_start)=(bsz, dim_emb)\n",
    "            h_t = h_t + self.PE[t+1].expand(bsz, self.dim_emb)\n",
    "            \n",
    "            # update tour\n",
    "            tours.append(idx)\n",
    "\n",
    "            # update masks with visited nodes\n",
    "            mask_visited_nodes = mask_visited_nodes.clone()\n",
    "            mask_visited_nodes[zero_to_bsz, idx] = True\n",
    "            \n",
    "            \n",
    "        # logprob_of_choices = sum_t log prob( pi_t | pi_(t-1),...,pi_0 )\n",
    "        sumLogProbOfActions = torch.stack(sumLogProbOfActions,dim=1).sum(dim=1) # size(sumLogProbOfActions)=(bsz,)\n",
    "\n",
    "        # convert the list of nodes into a tensor of shape (bsz,num_cities)\n",
    "        tours = torch.stack(tours,dim=1) # size(col_index)=(bsz, nb_nodes)\n",
    "        \n",
    "        return tours, sumLogProbOfActions\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "###################\n",
    "# Instantiate a training network and a baseline network\n",
    "###################\n",
    "try: \n",
    "    del model_train # remove existing model\n",
    "    del model_baseline # remove existing model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model_train = TSP_net(args.dim_input_nodes, args.dim_emb, args.dim_ff, \n",
    "              args.nb_layers_encoder, args.nb_layers_decoder, args.nb_heads, args.max_len_PE,\n",
    "              batchnorm=args.batchnorm)\n",
    "\n",
    "model_baseline = TSP_net(args.dim_input_nodes, args.dim_emb, args.dim_ff, \n",
    "              args.nb_layers_encoder, args.nb_layers_decoder, args.nb_heads, args.max_len_PE,\n",
    "              batchnorm=args.batchnorm)\n",
    "\n",
    "# uncomment these lines if trained with multiple GPUs\n",
    "print(torch.cuda.device_count())\n",
    "if torch.cuda.device_count()>1:\n",
    "    model_train = nn.DataParallel(model_train)\n",
    "    model_baseline = nn.DataParallel(model_baseline)\n",
    "# uncomment these lines if trained with multiple GPUs\n",
    "\n",
    "optimizer = torch.optim.Adam( model_train.parameters() , lr = args.lr ) \n",
    "\n",
    "model_train = model_train.to(device)\n",
    "model_baseline = model_baseline.to(device)\n",
    "model_baseline.eval()\n",
    "\n",
    "print(args); print('')\n",
    "\n",
    "# Logs\n",
    "os.system(\"mkdir logs\")\n",
    "time_stamp=datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\n",
    "file_name = 'logs'+'/'+time_stamp + \"-n{}\".format(args.nb_nodes) + \"-gpu{}\".format(args.gpu_id) + \".txt\"\n",
    "file = open(file_name,\"w\",1) \n",
    "file.write(time_stamp+'\\n\\n') \n",
    "for arg in vars(args):\n",
    "    file.write(arg)\n",
    "    hyper_param_val=\"={}\".format(getattr(args, arg))\n",
    "    file.write(hyper_param_val)\n",
    "    file.write('\\n')\n",
    "file.write('\\n\\n') \n",
    "plot_performance_train = []\n",
    "plot_performance_baseline = []\n",
    "all_strings = []\n",
    "epoch_ckpt = 0\n",
    "tot_time_ckpt = 0\n",
    "\n",
    "\n",
    "# # Uncomment these lines to re-start training with saved checkpoint\n",
    "# checkpoint_file = \"checkpoint/checkpoint_21-03-01--17-09-47-n100-gpu0,1.pkl\"\n",
    "# checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "# epoch_ckpt = checkpoint['epoch'] + 1\n",
    "# tot_time_ckpt = checkpoint['tot_time']\n",
    "# plot_performance_train = checkpoint['plot_performance_train']\n",
    "# plot_performance_baseline = checkpoint['plot_performance_baseline']\n",
    "# model_baseline.load_state_dict(checkpoint['model_baseline'])\n",
    "# model_train.load_state_dict(checkpoint['model_train'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "# print('Re-start training with saved checkpoint file={:s}\\n  Checkpoint at epoch= {:d} and time={:.3f}min\\n'.format(checkpoint_file,epoch_ckpt-1,tot_time_ckpt/60))\n",
    "# del checkpoint\n",
    "# # Uncomment these lines to re-start training with saved checkpoint\n",
    "\n",
    "\n",
    "###################\n",
    "# Main training loop \n",
    "###################\n",
    "torch.cuda.memory._record_memory_history()\n",
    "start_training_time = time.time()\n",
    "print('start train')\n",
    "for epoch in range(0,args.nb_epochs):\n",
    "    \n",
    "    # re-start training with saved checkpoint\n",
    "    epoch += epoch_ckpt\n",
    "\n",
    "    ###################\n",
    "    # Train model for one epoch\n",
    "    ###################\n",
    "    start = time.time()\n",
    "    model_train.train() \n",
    "\n",
    "    for step in range(1,args.nb_batch_per_epoch+1):    \n",
    "\n",
    "        # generate a batch of random TSP instances    \n",
    "        x = torch.rand(args.bsz, args.nb_nodes, args.dim_input_nodes, device=device) # size(x)=(bsz, nb_nodes, dim_input_nodes) \n",
    "\n",
    "        # compute tours for model\n",
    "        tour_train, sumLogProbOfActions =  model_train(x, deterministic=False) # size(tour_train)=(bsz, nb_nodes), size(sumLogProbOfActions)=(bsz)\n",
    "      \n",
    "        # compute tours for baseline\n",
    "        with torch.no_grad():\n",
    "            tour_baseline, _ = model_baseline(x, deterministic=True)\n",
    "\n",
    "        # get the lengths of the tours\n",
    "        L_train = compute_tour_length(x, tour_train) # size(L_train)=(bsz)\n",
    "        L_baseline = compute_tour_length(x, tour_baseline) # size(L_baseline)=(bsz)\n",
    "        \n",
    "        # backprop\n",
    "        loss = torch.mean( (L_train - L_baseline)* sumLogProbOfActions )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    time_one_epoch = time.time()-start\n",
    "    time_tot = time.time()-start_training_time + tot_time_ckpt\n",
    "\n",
    "        \n",
    "    ###################\n",
    "    # Evaluate train model and baseline on 10k random TSP instances\n",
    "    ###################\n",
    "    model_train.eval()\n",
    "    mean_tour_length_train = 0\n",
    "    mean_tour_length_baseline = 0\n",
    "    for step in range(0,args.nb_batch_eval):\n",
    "\n",
    "        # generate a batch of random tsp instances   \n",
    "        x = torch.rand(args.bsz, args.nb_nodes, args.dim_input_nodes, device=device) \n",
    "\n",
    "        # compute tour for model and baseline\n",
    "        with torch.no_grad():\n",
    "            tour_train, _ = model_train(x, deterministic=True)\n",
    "            tour_baseline, _ = model_baseline(x, deterministic=True)\n",
    "            \n",
    "        # get the lengths of the tours\n",
    "        L_train = compute_tour_length(x, tour_train)\n",
    "        L_baseline = compute_tour_length(x, tour_baseline)\n",
    "\n",
    "        # L_tr and L_bl are tensors of shape (bsz,). Compute the mean tour length\n",
    "        mean_tour_length_train += L_train.mean().item()\n",
    "        mean_tour_length_baseline += L_baseline.mean().item()\n",
    "\n",
    "    mean_tour_length_train =  mean_tour_length_train/ args.nb_batch_eval\n",
    "    mean_tour_length_baseline =  mean_tour_length_baseline/ args.nb_batch_eval\n",
    "\n",
    "    # evaluate train model and baseline and update if train model is better\n",
    "    update_baseline = mean_tour_length_train+args.tol < mean_tour_length_baseline\n",
    "    if update_baseline:\n",
    "        model_baseline.load_state_dict( model_train.state_dict() )\n",
    "\n",
    "    # Compute TSPs for small test set\n",
    "    # Note : this can be removed\n",
    "    with torch.no_grad():\n",
    "        tour_baseline, _ = model_baseline(x_1000tsp, deterministic=True)\n",
    "    mean_tour_length_test = compute_tour_length(x_1000tsp, tour_baseline).mean().item()\n",
    "    \n",
    "    # For checkpoint\n",
    "    plot_performance_train.append([ (epoch+1), mean_tour_length_train])\n",
    "    plot_performance_baseline.append([ (epoch+1), mean_tour_length_baseline])\n",
    "        \n",
    "    # Compute optimality gap\n",
    "    if args.nb_nodes==50: gap_train = mean_tour_length_train/5.692- 1.0\n",
    "    elif args.nb_nodes==100: gap_train = mean_tour_length_train/7.765- 1.0\n",
    "    else: gap_train = -1.0\n",
    "    \n",
    "    # Print and save in txt file\n",
    "    mystring_min = 'Epoch: {:d}, epoch time: {:.3f}min, tot time: {:.3f}day, L_train: {:.3f}, L_base: {:.3f}, L_test: {:.3f}, gap_train(%): {:.3f}, update: {}'.format(\n",
    "        epoch, time_one_epoch/60, time_tot/86400, mean_tour_length_train, mean_tour_length_baseline, mean_tour_length_test, 100*gap_train, update_baseline) \n",
    "    print(mystring_min) # Comment if plot display\n",
    "    file.write(mystring_min+'\\n')\n",
    "#     all_strings.append(mystring_min) # Uncomment if plot display\n",
    "#     for string in all_strings: \n",
    "#         print(string)\n",
    "    \n",
    "    # Saving checkpoint\n",
    "    checkpoint_dir = os.path.join(\"checkpoint\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'time': time_one_epoch,\n",
    "        'tot_time': time_tot,\n",
    "        'loss': loss.item(),\n",
    "        'TSP_length': [torch.mean(L_train).item(), torch.mean(L_baseline).item(), mean_tour_length_test],\n",
    "        'plot_performance_train': plot_performance_train,\n",
    "        'plot_performance_baseline': plot_performance_baseline,\n",
    "        'mean_tour_length_test': mean_tour_length_test,\n",
    "        'model_baseline': model_baseline.state_dict(),\n",
    "        'model_train': model_train.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        }, '{}.pkl'.format(checkpoint_dir + \"/checkpoint_\" + time_stamp + \"-n{}\".format(args.nb_nodes) + \"-gpu{}\".format(args.gpu_id)))\n",
    "\n",
    "torch.cuda.memory._dump_snapshot(\"memory.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2583.0350987911224\n",
      "2631.5465240932645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fec4b00cbb0>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "#read checkpoint\n",
    "if \"TSP\" not in os.getcwd():\n",
    "    os.chdir(\"TSP_Transformer\")\n",
    "checkpoint_file = \"checkpoint/checkpoint_21-03-01--17-09-47-n100-gpu0,1.pkl\"\n",
    "checkpoint = torch.load(checkpoint_file,map_location='cpu')\n",
    "print(checkpoint['time'])\n",
    "print(checkpoint['tot_time']/checkpoint['epoch'])\n",
    "plt.plot([x[0] for x in checkpoint['plot_performance_train']], [x[1] for x in checkpoint['plot_performance_train']], label='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
