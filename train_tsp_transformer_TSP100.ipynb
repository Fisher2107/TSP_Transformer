{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer Network for the Traveling Salesman Problem\n",
    "\n",
    "Xavier Bresson, Thomas Laurent, Feb 2021<br>\n",
    "\n",
    "Arxiv : https://arxiv.org/pdf/2103.03012.pdf<br>\n",
    "Talk : https://ipam.wistia.com/medias/0jrweluovs<br>\n",
    "Slides : https://t.co/ySxGiKtQL5<br>\n",
    "\n",
    "This code trains the transformer network by reinforcement learning.<br>\n",
    "Use the beam search code to test the trained network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18942/2912911153.py:18: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('png2x','pdf')\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# Libs\n",
    "###################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "# visualization \n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats, clear_output\n",
    "set_matplotlib_formats('png2x','pdf')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "try: \n",
    "    import networkx as nx\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    from concorde.tsp import TSPSolver # !pip install -e pyconcorde\n",
    "except:\n",
    "    pass\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/TSP_Transformer\n",
      "GPU name: Tesla T4, gpu_id: 0,1\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# Hardware : CPU / GPU(s)\n",
    "###################\n",
    "if \"TSP\" not in os.getcwd():\n",
    "    os.chdir(\"TSP_Transformer\")\n",
    "print(os.getcwd())\n",
    "device = torch.device(\"cpu\"); gpu_id = -1 # select CPU\n",
    "\n",
    "#gpu_id = '0' # select a single GPU  \n",
    "gpu_id = '0,1' # select multiple GPUs  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)  \n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('GPU name: {:s}, gpu_id: {:s}'.format(torch.cuda.get_device_name(0),gpu_id))   \n",
    "    \n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nb_nodes': 100, 'bsz': 256, 'dim_emb': 128, 'dim_ff': 512, 'dim_input_nodes': 2, 'nb_layers_encoder': 6, 'nb_layers_decoder': 2, 'nb_heads': 8, 'nb_epochs': 10000, 'nb_batch_per_epoch': 1500, 'nb_batch_eval': 20, 'gpu_id': '0,1', 'lr': 0.0001, 'tol': 0.001, 'batchnorm': True, 'max_len_PE': 1000}\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# Hyper-parameters\n",
    "###################\n",
    "\n",
    "class DotDict(dict):\n",
    "    def __init__(self, **kwds):\n",
    "        self.update(kwds)\n",
    "        self.__dict__ = self\n",
    "        \n",
    "args = DotDict()\n",
    "args.nb_nodes = 20 # TSP20\n",
    "args.nb_nodes = 50 # TSP50\n",
    "args.nb_nodes = 100 # TSP100\n",
    "args.bsz = 512 #512 TSP20 TSP50\n",
    "args.dim_emb = 128\n",
    "args.dim_ff = 512\n",
    "args.dim_input_nodes = 2\n",
    "args.nb_layers_encoder = 6\n",
    "args.nb_layers_decoder = 2\n",
    "args.nb_heads = 8\n",
    "args.nb_epochs = 10000\n",
    "args.nb_batch_per_epoch = 1500#2500\n",
    "args.nb_batch_eval = 20\n",
    "args.gpu_id = gpu_id\n",
    "args.lr = 1e-4\n",
    "args.tol = 1e-3\n",
    "args.batchnorm = True  # if batchnorm=True  than batch norm is used\n",
    "#args.batchnorm = False # if batchnorm=False than layer norm is used\n",
    "args.max_len_PE = 1000\n",
    "\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb of nodes : 100\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# Small test set for quick algorithm comparison\n",
    "# Note : this can be removed\n",
    "###################\n",
    "\n",
    "save_1000tsp = True\n",
    "save_1000tsp = False\n",
    "if save_1000tsp:\n",
    "    bsz = 1000\n",
    "    x = torch.rand(bsz, args.nb_nodes, args.dim_input_nodes, device='cpu') \n",
    "    print(x.size(),x[0])\n",
    "    data_dir = os.path.join(\"data\")\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    if args.nb_nodes==20 : torch.save({ 'x': x, }, '{}.pkl'.format(data_dir + \"/1000tsp20\"))\n",
    "    if args.nb_nodes==50 : torch.save({ 'x': x, }, '{}.pkl'.format(data_dir + \"/1000tsp50\"))\n",
    "    if args.nb_nodes==100 : torch.save({ 'x': x, }, '{}.pkl'.format(data_dir + \"/1000tsp100\"))\n",
    "\n",
    "checkpoint = None\n",
    "if args.nb_nodes==20 : checkpoint = torch.load(\"data/1000tsp20.pkl\")\n",
    "if args.nb_nodes==50 : checkpoint = torch.load(\"data/1000tsp50.pkl\")\n",
    "if args.nb_nodes==100 : checkpoint = torch.load(\"data/1000tsp100.pkl\")\n",
    "if checkpoint is not None:\n",
    "    x_1000tsp = checkpoint['x'].to(device)\n",
    "    n = x_1000tsp.size(1)\n",
    "    print('nb of nodes :',n)\n",
    "else:\n",
    "    x_1000tsp = torch.rand(1000, args.nb_nodes, args.dim_input_nodes, device='cpu')\n",
    "    n = x_1000tsp.size(1)\n",
    "    print('nb of nodes :',n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'nb_nodes': 100, 'bsz': 256, 'dim_emb': 128, 'dim_ff': 512, 'dim_input_nodes': 2, 'nb_layers_encoder': 6, 'nb_layers_decoder': 2, 'nb_heads': 8, 'nb_epochs': 10000, 'nb_batch_per_epoch': 1500, 'nb_batch_eval': 20, 'gpu_id': '0,1', 'lr': 0.0001, 'tol': 0.001, 'batchnorm': True, 'max_len_PE': 1000}\n",
      "\n",
      "start train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘logs’: File exists\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# Network definition\n",
    "# Notation : \n",
    "#            bsz : batch size\n",
    "#            nb_nodes : number of nodes/cities\n",
    "#            dim_emb : embedding/hidden dimension\n",
    "#            nb_heads : nb of attention heads\n",
    "#            dim_ff : feed-forward dimension\n",
    "#            nb_layers : number of encoder/decoder layers\n",
    "###################\n",
    "def compute_tour_length(x, tour): \n",
    "    \"\"\"\n",
    "    Compute the length of a batch of tours\n",
    "    Inputs : x of size (bsz, nb_nodes, 2) batch of tsp tour instances\n",
    "             tour of size (bsz, nb_nodes) batch of sequences (node indices) of tsp tours\n",
    "    Output : L of size (bsz,)             batch of lengths of each tsp tour\n",
    "    \"\"\"\n",
    "    bsz = x.shape[0]\n",
    "    nb_nodes = x.shape[1]\n",
    "    arange_vec = torch.arange(bsz, device=x.device)\n",
    "    first_cities = x[arange_vec, tour[:,0], :] # size(first_cities)=(bsz,2)\n",
    "    previous_cities = first_cities\n",
    "    L = torch.zeros(bsz, device=x.device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(1,nb_nodes):\n",
    "            current_cities = x[arange_vec, tour[:,i], :] \n",
    "            L += torch.sum( (current_cities - previous_cities)**2 , dim=1 )**0.5 # dist(current, previous node) \n",
    "            previous_cities = current_cities\n",
    "        L += torch.sum( (current_cities - first_cities)**2 , dim=1 )**0.5 # dist(last, first node)  \n",
    "    return L\n",
    "\n",
    "\n",
    "class Transformer_encoder_net(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder network based on self-attention transformer\n",
    "    Inputs :  \n",
    "      h of size      (bsz, nb_nodes+1, dim_emb)    batch of input cities\n",
    "    Outputs :  \n",
    "      h of size      (bsz, nb_nodes+1, dim_emb)    batch of encoded cities\n",
    "      score of size  (bsz, nb_nodes+1, nb_nodes+1) batch of attention scores\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_layers, dim_emb, nb_heads, dim_ff, batchnorm):\n",
    "        super(Transformer_encoder_net, self).__init__()\n",
    "        assert dim_emb == nb_heads* (dim_emb//nb_heads) # check if dim_emb is divisible by nb_heads\n",
    "        self.MHA_layers = nn.ModuleList( [nn.MultiheadAttention(dim_emb, nb_heads) for _ in range(nb_layers)] )\n",
    "        self.linear1_layers = nn.ModuleList( [nn.Linear(dim_emb, dim_ff) for _ in range(nb_layers)] )\n",
    "        self.linear2_layers = nn.ModuleList( [nn.Linear(dim_ff, dim_emb) for _ in range(nb_layers)] )   \n",
    "        if batchnorm:\n",
    "            self.norm1_layers = nn.ModuleList( [nn.BatchNorm1d(dim_emb) for _ in range(nb_layers)] )\n",
    "            self.norm2_layers = nn.ModuleList( [nn.BatchNorm1d(dim_emb) for _ in range(nb_layers)] )\n",
    "        else:\n",
    "            self.norm1_layers = nn.ModuleList( [nn.LayerNorm(dim_emb) for _ in range(nb_layers)] )\n",
    "            self.norm2_layers = nn.ModuleList( [nn.LayerNorm(dim_emb) for _ in range(nb_layers)] )\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_heads = nb_heads\n",
    "        self.batchnorm = batchnorm\n",
    "        \n",
    "    def forward(self, h):      \n",
    "        # PyTorch nn.MultiheadAttention requires input size (seq_len, bsz, dim_emb) \n",
    "        h = h.transpose(0,1) # size(h)=(nb_nodes, bsz, dim_emb)  \n",
    "        # L layers\n",
    "        for i in range(self.nb_layers):\n",
    "            h_rc = h # residual connection, size(h_rc)=(nb_nodes, bsz, dim_emb)\n",
    "            h, score = self.MHA_layers[i](h, h, h) # size(h)=(nb_nodes, bsz, dim_emb), size(score)=(bsz, nb_nodes, nb_nodes)\n",
    "            # add residual connection\n",
    "            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            if self.batchnorm:\n",
    "                # Pytorch nn.BatchNorm1d requires input size (bsz, dim, seq_len)\n",
    "                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = self.norm1_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            else:\n",
    "                h = self.norm1_layers[i](h)       # size(h)=(nb_nodes, bsz, dim_emb) \n",
    "            # feedforward\n",
    "            h_rc = h # residual connection\n",
    "            h = self.linear2_layers[i](torch.relu(self.linear1_layers[i](h)))\n",
    "            h = h_rc + h # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            if self.batchnorm:\n",
    "                h = h.permute(1,2,0).contiguous() # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = self.norm2_layers[i](h)       # size(h)=(bsz, dim_emb, nb_nodes)\n",
    "                h = h.permute(2,0,1).contiguous() # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "            else:\n",
    "                h = self.norm2_layers[i](h) # size(h)=(nb_nodes, bsz, dim_emb)\n",
    "        # Transpose h\n",
    "        h = h.transpose(0,1) # size(h)=(bsz, nb_nodes, dim_emb)\n",
    "        return h, score\n",
    "    \n",
    "\n",
    "def myMHA(Q, K, V, nb_heads, mask=None, clip_value=None):\n",
    "    \"\"\"\n",
    "    Compute multi-head attention (MHA) given a query Q, key K, value V and attention mask :\n",
    "      h = Concat_{k=1}^nb_heads softmax(Q_k^T.K_k).V_k \n",
    "    Note : We did not use nn.MultiheadAttention to avoid re-computing all linear transformations at each call.\n",
    "    Inputs : Q of size (bsz, dim_emb, 1)                batch of queries\n",
    "             K of size (bsz, dim_emb, nb_nodes+1)       batch of keys\n",
    "             V of size (bsz, dim_emb, nb_nodes+1)       batch of values\n",
    "             mask of size (bsz, nb_nodes+1)             batch of masks of visited cities\n",
    "             clip_value is a scalar \n",
    "    Outputs : attn_output of size (bsz, 1, dim_emb)     batch of attention vectors\n",
    "              attn_weights of size (bsz, 1, nb_nodes+1) batch of attention weights\n",
    "    \"\"\"\n",
    "    bsz, nb_nodes, emd_dim = K.size() #  dim_emb must be divisable by nb_heads\n",
    "    if nb_heads>1:\n",
    "        # PyTorch view requires contiguous dimensions for correct reshaping\n",
    "        Q = Q.transpose(1,2).contiguous() # size(Q)=(bsz, dim_emb, 1)\n",
    "        Q = Q.view(bsz*nb_heads, emd_dim//nb_heads, 1) # size(Q)=(bsz*nb_heads, dim_emb//nb_heads, 1)\n",
    "        Q = Q.transpose(1,2).contiguous() # size(Q)=(bsz*nb_heads, 1, dim_emb//nb_heads)\n",
    "        K = K.transpose(1,2).contiguous() # size(K)=(bsz, dim_emb, nb_nodes+1)\n",
    "        K = K.view(bsz*nb_heads, emd_dim//nb_heads, nb_nodes) # size(K)=(bsz*nb_heads, dim_emb//nb_heads, nb_nodes+1)\n",
    "        K = K.transpose(1,2).contiguous() # size(K)=(bsz*nb_heads, nb_nodes+1, dim_emb//nb_heads)\n",
    "        V = V.transpose(1,2).contiguous() # size(V)=(bsz, dim_emb, nb_nodes+1)\n",
    "        V = V.view(bsz*nb_heads, emd_dim//nb_heads, nb_nodes) # size(V)=(bsz*nb_heads, dim_emb//nb_heads, nb_nodes+1)\n",
    "        V = V.transpose(1,2).contiguous() # size(V)=(bsz*nb_heads, nb_nodes+1, dim_emb//nb_heads)\n",
    "    attn_weights = torch.bmm(Q, K.transpose(1,2))/ Q.size(-1)**0.5 # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "    if clip_value is not None:\n",
    "        attn_weights = clip_value * torch.tanh(attn_weights)\n",
    "    if mask is not None:\n",
    "        if nb_heads>1:\n",
    "            mask = torch.repeat_interleave(mask, repeats=nb_heads, dim=0) # size(mask)=(bsz*nb_heads, nb_nodes+1)\n",
    "        #attn_weights = attn_weights.masked_fill(mask.unsqueeze(1), float('-inf')) # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "        attn_weights = attn_weights.masked_fill(mask.unsqueeze(1), float('-1e9')) # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "    attn_weights = torch.softmax(attn_weights, dim=-1) # size(attn_weights)=(bsz*nb_heads, 1, nb_nodes+1)\n",
    "    attn_output = torch.bmm(attn_weights, V) # size(attn_output)=(bsz*nb_heads, 1, dim_emb//nb_heads)\n",
    "    if nb_heads>1:\n",
    "        attn_output = attn_output.transpose(1,2).contiguous() # size(attn_output)=(bsz*nb_heads, dim_emb//nb_heads, 1)\n",
    "        attn_output = attn_output.view(bsz, emd_dim, 1) # size(attn_output)=(bsz, dim_emb, 1)\n",
    "        attn_output = attn_output.transpose(1,2).contiguous() # size(attn_output)=(bsz, 1, dim_emb)\n",
    "        attn_weights = attn_weights.view(bsz, nb_heads, 1, nb_nodes) # size(attn_weights)=(bsz, nb_heads, 1, nb_nodes+1)\n",
    "        attn_weights = attn_weights.mean(dim=1) # mean over the heads, size(attn_weights)=(bsz, 1, nb_nodes+1)\n",
    "    return attn_output, attn_weights\n",
    "    \n",
    "    \n",
    "class AutoRegressiveDecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single decoder layer based on self-attention and query-attention\n",
    "    Inputs :  \n",
    "      h_t of size      (bsz, 1, dim_emb)          batch of input queries\n",
    "      K_att of size    (bsz, nb_nodes+1, dim_emb) batch of query-attention keys\n",
    "      V_att of size    (bsz, nb_nodes+1, dim_emb) batch of query-attention values\n",
    "      mask of size     (bsz, nb_nodes+1)          batch of masks of visited cities\n",
    "    Output :  \n",
    "      h_t of size (bsz, nb_nodes+1)               batch of transformed queries\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_emb, nb_heads):\n",
    "        super(AutoRegressiveDecoderLayer, self).__init__()\n",
    "        self.dim_emb = dim_emb\n",
    "        self.nb_heads = nb_heads\n",
    "        self.Wq_selfatt = nn.Linear(dim_emb, dim_emb)\n",
    "        self.Wk_selfatt = nn.Linear(dim_emb, dim_emb)\n",
    "        self.Wv_selfatt = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W0_selfatt = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W0_att = nn.Linear(dim_emb, dim_emb)\n",
    "        self.Wq_att = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W1_MLP = nn.Linear(dim_emb, dim_emb)\n",
    "        self.W2_MLP = nn.Linear(dim_emb, dim_emb)\n",
    "        self.BN_selfatt = nn.LayerNorm(dim_emb)\n",
    "        self.BN_att = nn.LayerNorm(dim_emb)\n",
    "        self.BN_MLP = nn.LayerNorm(dim_emb)\n",
    "        self.K_sa = None\n",
    "        self.V_sa = None\n",
    "\n",
    "    def reset_selfatt_keys_values(self):\n",
    "        self.K_sa = None\n",
    "        self.V_sa = None\n",
    "        \n",
    "    def forward(self, h_t, K_att, V_att, mask):\n",
    "        bsz = h_t.size(0)\n",
    "        h_t = h_t.view(bsz,1,self.dim_emb) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        # embed the query for self-attention\n",
    "        q_sa = self.Wq_selfatt(h_t) # size(q_sa)=(bsz, 1, dim_emb)\n",
    "        k_sa = self.Wk_selfatt(h_t) # size(k_sa)=(bsz, 1, dim_emb)\n",
    "        v_sa = self.Wv_selfatt(h_t) # size(v_sa)=(bsz, 1, dim_emb)\n",
    "        # concatenate the new self-attention key and value to the previous keys and values\n",
    "        if self.K_sa is None:\n",
    "            self.K_sa = k_sa # size(self.K_sa)=(bsz, 1, dim_emb)\n",
    "            self.V_sa = v_sa # size(self.V_sa)=(bsz, 1, dim_emb)\n",
    "        else:\n",
    "            self.K_sa = torch.cat([self.K_sa, k_sa], dim=1)\n",
    "            self.V_sa = torch.cat([self.V_sa, v_sa], dim=1)\n",
    "        # compute self-attention between nodes in the partial tour\n",
    "        h_t = h_t + self.W0_selfatt( myMHA(q_sa, self.K_sa, self.V_sa, self.nb_heads)[0] ) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        h_t = self.BN_selfatt(h_t.squeeze()) # size(h_t)=(bsz, dim_emb)\n",
    "        h_t = h_t.view(bsz, 1, self.dim_emb) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        # compute attention between self-attention nodes and encoding nodes in the partial tour (translation process)\n",
    "        q_a = self.Wq_att(h_t) # size(q_a)=(bsz, 1, dim_emb)\n",
    "        h_t = h_t + self.W0_att( myMHA(q_a, K_att, V_att, self.nb_heads, mask)[0] ) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        h_t = self.BN_att(h_t.squeeze()) # size(h_t)=(bsz, dim_emb)\n",
    "        h_t = h_t.view(bsz, 1, self.dim_emb) # size(h_t)=(bsz, 1, dim_emb)\n",
    "        # MLP\n",
    "        h_t = h_t + self.W2_MLP(torch.relu(self.W1_MLP(h_t)))\n",
    "        h_t = self.BN_MLP(h_t.squeeze(1)) # size(h_t)=(bsz, dim_emb)\n",
    "        return h_t\n",
    "        \n",
    "        \n",
    "class Transformer_decoder_net(nn.Module): \n",
    "    \"\"\"\n",
    "    Decoder network based on self-attention and query-attention transformers\n",
    "    Inputs :  \n",
    "      h_t of size      (bsz, 1, dim_emb)                            batch of input queries\n",
    "      K_att of size    (bsz, nb_nodes+1, dim_emb*nb_layers_decoder) batch of query-attention keys for all decoding layers\n",
    "      V_att of size    (bsz, nb_nodes+1, dim_emb*nb_layers_decoder) batch of query-attention values for all decoding layers\n",
    "      mask of size     (bsz, nb_nodes+1)                            batch of masks of visited cities\n",
    "    Output :  \n",
    "      prob_next_node of size (bsz, nb_nodes+1)                      batch of probabilities of next node\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_emb, nb_heads, nb_layers_decoder):\n",
    "        super(Transformer_decoder_net, self).__init__()\n",
    "        self.dim_emb = dim_emb\n",
    "        self.nb_heads = nb_heads\n",
    "        self.nb_layers_decoder = nb_layers_decoder\n",
    "        self.decoder_layers = nn.ModuleList( [AutoRegressiveDecoderLayer(dim_emb, nb_heads) for _ in range(nb_layers_decoder-1)] )\n",
    "        self.Wq_final = nn.Linear(dim_emb, dim_emb)\n",
    "        \n",
    "    # Reset to None self-attention keys and values when decoding starts \n",
    "    def reset_selfatt_keys_values(self): \n",
    "        for l in range(self.nb_layers_decoder-1):\n",
    "            self.decoder_layers[l].reset_selfatt_keys_values()\n",
    "            \n",
    "    def forward(self, h_t, K_att, V_att, mask):\n",
    "        for l in range(self.nb_layers_decoder):\n",
    "            K_att_l = K_att[:,:,l*self.dim_emb:(l+1)*self.dim_emb].contiguous()  # size(K_att_l)=(bsz, nb_nodes+1, dim_emb)\n",
    "            V_att_l = V_att[:,:,l*self.dim_emb:(l+1)*self.dim_emb].contiguous()  # size(V_att_l)=(bsz, nb_nodes+1, dim_emb)\n",
    "            if l<self.nb_layers_decoder-1: # decoder layers with multiple heads (intermediate layers)\n",
    "                h_t = self.decoder_layers[l](h_t, K_att_l, V_att_l, mask)\n",
    "            else: # decoder layers with single head (final layer)\n",
    "                q_final = self.Wq_final(h_t)\n",
    "                bsz = h_t.size(0)\n",
    "                q_final = q_final.view(bsz, 1, self.dim_emb)\n",
    "                attn_weights = myMHA(q_final, K_att_l, V_att_l, 1, mask, 10)[1] \n",
    "        prob_next_node = attn_weights.squeeze(1) \n",
    "        return prob_next_node\n",
    "\n",
    "\n",
    "def generate_positional_encoding(d_model, max_len):\n",
    "    \"\"\"\n",
    "    Create standard transformer PEs.\n",
    "    Inputs :  \n",
    "      d_model is a scalar correspoding to the hidden dimension\n",
    "      max_len is the maximum length of the sequence\n",
    "    Output :  \n",
    "      pe of size (max_len, d_model), where d_model=dim_emb, max_len=1000\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "    pe[:,0::2] = torch.sin(position * div_term)\n",
    "    pe[:,1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "    \n",
    "    \n",
    "class TSP_net(nn.Module): \n",
    "    \"\"\"\n",
    "    The TSP network is composed of two steps :\n",
    "      Step 1. Encoder step : Take a set of 2D points representing a fully connected graph \n",
    "                             and encode the set with self-transformer.\n",
    "      Step 2. Decoder step : Build the TSP tour recursively/autoregressively, \n",
    "                             i.e. one node at a time, with a self-transformer and query-transformer. \n",
    "    Inputs : \n",
    "      x of size (bsz, nb_nodes, dim_emb) Euclidian coordinates of the nodes/cities\n",
    "      deterministic is a boolean : If True the salesman will chose the city with highest probability. \n",
    "                                   If False the salesman will chose the city with Bernouilli sampling.\n",
    "    Outputs : \n",
    "      tours of size (bsz, nb_nodes) : batch of tours, i.e. sequences of ordered cities \n",
    "                                      tours[b,t] contains the idx of the city visited at step t in batch b\n",
    "      sumLogProbOfActions of size (bsz,) : batch of sum_t log prob( pi_t | pi_(t-1),...,pi_0 )\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim_input_nodes, dim_emb, dim_ff, nb_layers_encoder, nb_layers_decoder, nb_heads, max_len_PE,\n",
    "                 batchnorm=True):\n",
    "        super(TSP_net, self).__init__()\n",
    "        \n",
    "        self.dim_emb = dim_emb\n",
    "        \n",
    "        # input embedding layer\n",
    "        self.input_emb = nn.Linear(dim_input_nodes, dim_emb)\n",
    "        \n",
    "        # encoder layer\n",
    "        self.encoder = Transformer_encoder_net(nb_layers_encoder, dim_emb, nb_heads, dim_ff, batchnorm)\n",
    "        \n",
    "        # vector to start decoding \n",
    "        self.start_placeholder = nn.Parameter(torch.randn(dim_emb))\n",
    "        \n",
    "        # decoder layer\n",
    "        self.decoder = Transformer_decoder_net(dim_emb, nb_heads, nb_layers_decoder)\n",
    "        self.WK_att_decoder = nn.Linear(dim_emb, nb_layers_decoder* dim_emb) \n",
    "        self.WV_att_decoder = nn.Linear(dim_emb, nb_layers_decoder* dim_emb) \n",
    "        self.PE = generate_positional_encoding(dim_emb, max_len_PE)        \n",
    "        \n",
    "    def forward(self, x, deterministic=False):\n",
    "\n",
    "        # some parameters\n",
    "        bsz = x.shape[0]\n",
    "        nb_nodes = x.shape[1]\n",
    "        zero_to_bsz = torch.arange(bsz, device=x.device) # [0,1,...,bsz-1]\n",
    "\n",
    "        # input embedding layer\n",
    "        h = self.input_emb(x) # size(h)=(bsz, nb_nodes, dim_emb)\n",
    "        \n",
    "        # concat the nodes and the input placeholder that starts the decoding\n",
    "        h = torch.cat([h, self.start_placeholder.repeat(bsz, 1, 1)], dim=1) # size(start_placeholder)=(bsz, nb_nodes+1, dim_emb)\n",
    "        \n",
    "        # encoder layer\n",
    "        h_encoder, _ = self.encoder(h) # size(h)=(bsz, nb_nodes+1, dim_emb)\n",
    "\n",
    "        # list that will contain Long tensors of shape (bsz,) that gives the idx of the cities chosen at time t\n",
    "        tours = []\n",
    "\n",
    "        # list that will contain Float tensors of shape (bsz,) that gives the neg log probs of the choices made at time t\n",
    "        sumLogProbOfActions = []\n",
    "\n",
    "        # key and value for decoder    \n",
    "        K_att_decoder = self.WK_att_decoder(h_encoder) # size(K_att)=(bsz, nb_nodes+1, dim_emb*nb_layers_decoder)\n",
    "        V_att_decoder = self.WV_att_decoder(h_encoder) # size(V_att)=(bsz, nb_nodes+1, dim_emb*nb_layers_decoder)\n",
    "        \n",
    "        # input placeholder that starts the decoding\n",
    "        self.PE = self.PE.to(x.device)\n",
    "        idx_start_placeholder = torch.Tensor([nb_nodes]).long().repeat(bsz).to(x.device)\n",
    "        h_start = h_encoder[zero_to_bsz, idx_start_placeholder, :] + self.PE[0].repeat(bsz,1) # size(h_start)=(bsz, dim_emb)\n",
    "        \n",
    "        # initialize mask of visited cities\n",
    "        mask_visited_nodes = torch.zeros(bsz, nb_nodes+1, device=x.device).bool() # False\n",
    "        mask_visited_nodes[zero_to_bsz, idx_start_placeholder] = True\n",
    "        \n",
    "        # clear key and val stored in the decoder\n",
    "        self.decoder.reset_selfatt_keys_values()\n",
    "\n",
    "        # construct tour recursively\n",
    "        h_t = h_start\n",
    "        for t in range(nb_nodes):\n",
    "            \n",
    "            # compute probability over the next node in the tour\n",
    "            prob_next_node = self.decoder(h_t, K_att_decoder, V_att_decoder, mask_visited_nodes) # size(prob_next_node)=(bsz, nb_nodes+1)\n",
    "            \n",
    "            # choose node with highest probability or sample with Bernouilli \n",
    "            if deterministic:\n",
    "                idx = torch.argmax(prob_next_node, dim=1) # size(query)=(bsz,)\n",
    "            else:\n",
    "                idx = Categorical(prob_next_node).sample() # size(query)=(bsz,)\n",
    "            \n",
    "            # compute logprobs of the action items in the list sumLogProbOfActions   \n",
    "            ProbOfChoices = prob_next_node[zero_to_bsz, idx] \n",
    "            sumLogProbOfActions.append( torch.log(ProbOfChoices) )  # size(query)=(bsz,)\n",
    "\n",
    "            # update embedding of the current visited node\n",
    "            h_t = h_encoder[zero_to_bsz, idx, :] # size(h_start)=(bsz, dim_emb)\n",
    "            h_t = h_t + self.PE[t+1].expand(bsz, self.dim_emb)\n",
    "            \n",
    "            # update tour\n",
    "            tours.append(idx)\n",
    "\n",
    "            # update masks with visited nodes\n",
    "            mask_visited_nodes = mask_visited_nodes.clone()\n",
    "            mask_visited_nodes[zero_to_bsz, idx] = True\n",
    "            \n",
    "            \n",
    "        # logprob_of_choices = sum_t log prob( pi_t | pi_(t-1),...,pi_0 )\n",
    "        sumLogProbOfActions = torch.stack(sumLogProbOfActions,dim=1).sum(dim=1) # size(sumLogProbOfActions)=(bsz,)\n",
    "\n",
    "        # convert the list of nodes into a tensor of shape (bsz,num_cities)\n",
    "        tours = torch.stack(tours,dim=1) # size(col_index)=(bsz, nb_nodes)\n",
    "        \n",
    "        return tours, sumLogProbOfActions\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "###################\n",
    "# Instantiate a training network and a baseline network\n",
    "###################\n",
    "try: \n",
    "    del model_train # remove existing model\n",
    "    del model_baseline # remove existing model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model_train = TSP_net(args.dim_input_nodes, args.dim_emb, args.dim_ff, \n",
    "              args.nb_layers_encoder, args.nb_layers_decoder, args.nb_heads, args.max_len_PE,\n",
    "              batchnorm=args.batchnorm)\n",
    "\n",
    "model_baseline = TSP_net(args.dim_input_nodes, args.dim_emb, args.dim_ff, \n",
    "              args.nb_layers_encoder, args.nb_layers_decoder, args.nb_heads, args.max_len_PE,\n",
    "              batchnorm=args.batchnorm)\n",
    "\n",
    "# uncomment these lines if trained with multiple GPUs\n",
    "print(torch.cuda.device_count())\n",
    "if torch.cuda.device_count()>1:\n",
    "    model_train = nn.DataParallel(model_train)\n",
    "    model_baseline = nn.DataParallel(model_baseline)\n",
    "# uncomment these lines if trained with multiple GPUs\n",
    "\n",
    "optimizer = torch.optim.Adam( model_train.parameters() , lr = args.lr ) \n",
    "\n",
    "model_train = model_train.to(device)\n",
    "model_baseline = model_baseline.to(device)\n",
    "model_baseline.eval()\n",
    "\n",
    "print(args); print('')\n",
    "\n",
    "# Logs\n",
    "os.system(\"mkdir logs\")\n",
    "time_stamp=datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\n",
    "file_name = 'logs'+'/'+time_stamp + \"-n{}\".format(args.nb_nodes) + \"-gpu{}\".format(args.gpu_id) + \".txt\"\n",
    "file = open(file_name,\"w\",1) \n",
    "file.write(time_stamp+'\\n\\n') \n",
    "for arg in vars(args):\n",
    "    file.write(arg)\n",
    "    hyper_param_val=\"={}\".format(getattr(args, arg))\n",
    "    file.write(hyper_param_val)\n",
    "    file.write('\\n')\n",
    "file.write('\\n\\n') \n",
    "plot_performance_train = []\n",
    "plot_performance_baseline = []\n",
    "all_strings = []\n",
    "epoch_ckpt = 0\n",
    "tot_time_ckpt = 0\n",
    "\n",
    "\n",
    "# # Uncomment these lines to re-start training with saved checkpoint\n",
    "# checkpoint_file = \"checkpoint/checkpoint_21-03-01--17-09-47-n100-gpu0,1.pkl\"\n",
    "# checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "# epoch_ckpt = checkpoint['epoch'] + 1\n",
    "# tot_time_ckpt = checkpoint['tot_time']\n",
    "# plot_performance_train = checkpoint['plot_performance_train']\n",
    "# plot_performance_baseline = checkpoint['plot_performance_baseline']\n",
    "# model_baseline.load_state_dict(checkpoint['model_baseline'])\n",
    "# model_train.load_state_dict(checkpoint['model_train'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "# print('Re-start training with saved checkpoint file={:s}\\n  Checkpoint at epoch= {:d} and time={:.3f}min\\n'.format(checkpoint_file,epoch_ckpt-1,tot_time_ckpt/60))\n",
    "# del checkpoint\n",
    "# # Uncomment these lines to re-start training with saved checkpoint\n",
    "\n",
    "\n",
    "###################\n",
    "# Main training loop \n",
    "###################\n",
    "torch.cuda.memory._record_memory_history()\n",
    "start_training_time = time.time()\n",
    "print('start train')\n",
    "for epoch in range(0,args.nb_epochs):\n",
    "    \n",
    "    # re-start training with saved checkpoint\n",
    "    epoch += epoch_ckpt\n",
    "\n",
    "    ###################\n",
    "    # Train model for one epoch\n",
    "    ###################\n",
    "    start = time.time()\n",
    "    model_train.train() \n",
    "\n",
    "    for step in range(1,args.nb_batch_per_epoch+1):    \n",
    "\n",
    "        # generate a batch of random TSP instances    \n",
    "        x = torch.rand(args.bsz, args.nb_nodes, args.dim_input_nodes, device=device) # size(x)=(bsz, nb_nodes, dim_input_nodes) \n",
    "\n",
    "        # compute tours for model\n",
    "        tour_train, sumLogProbOfActions =  model_train(x, deterministic=False) # size(tour_train)=(bsz, nb_nodes), size(sumLogProbOfActions)=(bsz)\n",
    "      \n",
    "        # compute tours for baseline\n",
    "        with torch.no_grad():\n",
    "            tour_baseline, _ = model_baseline(x, deterministic=True)\n",
    "\n",
    "        # get the lengths of the tours\n",
    "        L_train = compute_tour_length(x, tour_train) # size(L_train)=(bsz)\n",
    "        L_baseline = compute_tour_length(x, tour_baseline) # size(L_baseline)=(bsz)\n",
    "        \n",
    "        # backprop\n",
    "        loss = torch.mean( (L_train - L_baseline)* sumLogProbOfActions )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    time_one_epoch = time.time()-start\n",
    "    time_tot = time.time()-start_training_time + tot_time_ckpt\n",
    "\n",
    "        \n",
    "    ###################\n",
    "    # Evaluate train model and baseline on 10k random TSP instances\n",
    "    ###################\n",
    "    model_train.eval()\n",
    "    mean_tour_length_train = 0\n",
    "    mean_tour_length_baseline = 0\n",
    "    for step in range(0,args.nb_batch_eval):\n",
    "\n",
    "        # generate a batch of random tsp instances   \n",
    "        x = torch.rand(args.bsz, args.nb_nodes, args.dim_input_nodes, device=device) \n",
    "\n",
    "        # compute tour for model and baseline\n",
    "        with torch.no_grad():\n",
    "            tour_train, _ = model_train(x, deterministic=True)\n",
    "            tour_baseline, _ = model_baseline(x, deterministic=True)\n",
    "            \n",
    "        # get the lengths of the tours\n",
    "        L_train = compute_tour_length(x, tour_train)\n",
    "        L_baseline = compute_tour_length(x, tour_baseline)\n",
    "\n",
    "        # L_tr and L_bl are tensors of shape (bsz,). Compute the mean tour length\n",
    "        mean_tour_length_train += L_train.mean().item()\n",
    "        mean_tour_length_baseline += L_baseline.mean().item()\n",
    "\n",
    "    mean_tour_length_train =  mean_tour_length_train/ args.nb_batch_eval\n",
    "    mean_tour_length_baseline =  mean_tour_length_baseline/ args.nb_batch_eval\n",
    "\n",
    "    # evaluate train model and baseline and update if train model is better\n",
    "    update_baseline = mean_tour_length_train+args.tol < mean_tour_length_baseline\n",
    "    if update_baseline:\n",
    "        model_baseline.load_state_dict( model_train.state_dict() )\n",
    "\n",
    "    # Compute TSPs for small test set\n",
    "    # Note : this can be removed\n",
    "    with torch.no_grad():\n",
    "        tour_baseline, _ = model_baseline(x_1000tsp, deterministic=True)\n",
    "    mean_tour_length_test = compute_tour_length(x_1000tsp, tour_baseline).mean().item()\n",
    "    \n",
    "    # For checkpoint\n",
    "    plot_performance_train.append([ (epoch+1), mean_tour_length_train])\n",
    "    plot_performance_baseline.append([ (epoch+1), mean_tour_length_baseline])\n",
    "        \n",
    "    # Compute optimality gap\n",
    "    if args.nb_nodes==50: gap_train = mean_tour_length_train/5.692- 1.0\n",
    "    elif args.nb_nodes==100: gap_train = mean_tour_length_train/7.765- 1.0\n",
    "    else: gap_train = -1.0\n",
    "    \n",
    "    # Print and save in txt file\n",
    "    mystring_min = 'Epoch: {:d}, epoch time: {:.3f}min, tot time: {:.3f}day, L_train: {:.3f}, L_base: {:.3f}, L_test: {:.3f}, gap_train(%): {:.3f}, update: {}'.format(\n",
    "        epoch, time_one_epoch/60, time_tot/86400, mean_tour_length_train, mean_tour_length_baseline, mean_tour_length_test, 100*gap_train, update_baseline) \n",
    "    print(mystring_min) # Comment if plot display\n",
    "    file.write(mystring_min+'\\n')\n",
    "#     all_strings.append(mystring_min) # Uncomment if plot display\n",
    "#     for string in all_strings: \n",
    "#         print(string)\n",
    "    \n",
    "    # Saving checkpoint\n",
    "    checkpoint_dir = os.path.join(\"checkpoint\")\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'time': time_one_epoch,\n",
    "        'tot_time': time_tot,\n",
    "        'loss': loss.item(),\n",
    "        'TSP_length': [torch.mean(L_train).item(), torch.mean(L_baseline).item(), mean_tour_length_test],\n",
    "        'plot_performance_train': plot_performance_train,\n",
    "        'plot_performance_baseline': plot_performance_baseline,\n",
    "        'mean_tour_length_test': mean_tour_length_test,\n",
    "        'model_baseline': model_baseline.state_dict(),\n",
    "        'model_train': model_train.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        }, '{}.pkl'.format(checkpoint_dir + \"/checkpoint_\" + time_stamp + \"-n{}\".format(args.nb_nodes) + \"-gpu{}\".format(args.gpu_id)))\n",
    "\n",
    "torch.cuda.memory._dump_snapshot(\"memory.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "882\n",
      "2583.0350987911224\n",
      "2631.5465240932645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcf7e898740>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "#read checkpoint\n",
    "if \"TSP\" not in os.getcwd():\n",
    "    os.chdir(\"TSP_Transformer\")\n",
    "checkpoint_file = \"checkpoint/checkpoint_21-03-01--17-09-47-n100-gpu0,1.pkl\"\n",
    "checkpoint = torch.load(checkpoint_file,map_location='cpu')\n",
    "print('epoch =',checkpoint['epoch'])\n",
    "print('time = ',checkpoint['time'], 'average = ', checkpoint['tot_time']/checkpoint['epoch'])\n",
    "print('loss = ',checkpoint['loss'])\n",
    "print('TSP_length = ',checkpoint['TSP_length'])\n",
    "plt.plot([x[0] for x in checkpoint['plot_performance_train']], [x[1] for x in checkpoint['plot_performance_train']], label='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
